{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food101 Classification using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-image\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.0a0+056cfaf\n",
      "Torchvision Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import collections\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"/food101/images/\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"densenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 101\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 4\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 50\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_ix = {}\n",
    "ix_to_class = {}\n",
    "with open('/food101/meta/classes.txt', 'r') as txt:\n",
    "    classes = [l.strip() for l in txt.readlines()]\n",
    "    class_to_ix = dict(zip(classes, range(len(classes))))\n",
    "    ix_to_class = dict(zip(range(len(classes)), classes))\n",
    "    class_to_ix = {v: k for k, v in ix_to_class.items()}\n",
    "sorted_class_to_ix = collections.OrderedDict(sorted(class_to_ix.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/food101/meta/train.json') as f:\n",
    "    train_files = json.load(f)\n",
    "\n",
    "with open('/food101/meta/test.json') as f:\n",
    "    test_files = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = {'filename':[], 'label':[]}\n",
    "df_test = {'filename':[], 'label':[]}\n",
    "for class_name in train_files:\n",
    "    for i in range(len(train_files[class_name])):\n",
    "        df_train['filename'].append('/food101/images/' + str(train_files[class_name][i]) + '.jpg')\n",
    "        df_train['label'].append(class_to_ix[class_name])\n",
    "df_train = pd.DataFrame(df_train)\n",
    "\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.1)\n",
    "df_train.reset_index(inplace=True)\n",
    "df_valid.reset_index(inplace=True)\n",
    "df_train.drop(columns=['index'], inplace=True)\n",
    "df_valid.drop(columns=['index'], inplace=True)\n",
    "\n",
    "for class_name in test_files:\n",
    "    for i in range(len(test_files[class_name])):\n",
    "        df_test['filename'].append('/food101/images/' + str(test_files[class_name][i]) + '.jpg')\n",
    "        df_test['label'].append(class_to_ix[class_name])\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataframeDataset(Dataset):\n",
    "    \"\"\"Create dataset using pandas dataframe. The dataframe needs to have image filename and category label columns.\"\"\"\n",
    "\n",
    "    def __init__(self, df=None,is_csv=False, csv_file=None,is_full_path=True,root_dir=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        if is_csv:\n",
    "            self.image_data = pd.read_csv(csv_file)\n",
    "        else:\n",
    "            self.image_data = df\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.is_full_path = is_full_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_full_path == False:\n",
    "            img_name = os.path.join(self.root_dir,\n",
    "                                    self.image_data.iloc[idx, 0])\n",
    "        else:\n",
    "            img_name = self.image_data.iloc[idx, 0]\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB') #Some images are BW, which throws RuntimeError, so ensure all images are RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.image_data.iloc[idx, 1]\n",
    "#         sample = {'image': image, 'label': label}\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions\n",
    "----------------\n",
    "\n",
    "Before we write the code for adjusting the models, lets define a few\n",
    "helper functions.\n",
    "\n",
    "Model Training and Validation Code\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The __train_model__ function handles the training and validation of a\n",
    "given model. As input, it takes a PyTorch model, a dictionary of\n",
    "dataloaders, a loss function, an optimizer, a specified number of epochs\n",
    "to train and validate for, and a boolean flag for when the model is an\n",
    "Inception model. \n",
    "\n",
    "The __is_inception__ flag is used to accomodate the *Inception v3* model, as that architecture uses an auxiliary output and the overall model loss respects both the auxiliary output and the final output, as described here <https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958>\n",
    "\n",
    "The function trains for the specified number of epochs and after each epoch runs a full validation step. It also keeps track of the best performing model (in terms of validation accuracy), and at the end of training returns the best performing model. After each epoch, the training and validation accuracies are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "#                 inputs = sample['image']\n",
    "#                 labels = sample['label']\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Model Parameters’ .requires_grad attribute\n",
    "\n",
    "This helper function sets the ``.requires_grad`` attribute of the\n",
    "parameters in the model to False when we are feature extracting. By\n",
    "default, when we load a pretrained model all of the parameters have\n",
    "``.requires_grad=True``, which is fine if we are training from scratch\n",
    "or finetuning. However, if we are feature extracting and only want to\n",
    "compute gradients for the newly initialized layer then we want all of\n",
    "the other parameters to not require gradients. This will make more sense\n",
    "later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and Reshape the Networks\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=101, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n",
    "Now that we know what the input size must be, we can initialize the data\n",
    "transforms, image datasets, and the dataloaders. Notice, the models were\n",
    "pretrained with the hard-coded normalization values, as described\n",
    "`here <https://pytorch.org/docs/master/torchvision/models.html>`__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "dataframe_dicts = {\n",
    "    'train':df_train,\n",
    "    'val':df_valid,\n",
    "    'test':df_test\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training, validation and test datasets\n",
    "image_datasets = {x: PandasDataframeDataset(df=dataframe_dicts[x], transform=data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Optimizer\n",
    "--------------------\n",
    "\n",
    "Now that the model structure is correct, the final step for finetuning\n",
    "and feature extracting is to create an optimizer that only updates the\n",
    "desired parameters. Recall that after loading the pretrained model, but\n",
    "before reshaping, if ``feature_extract=True`` we manually set all of the\n",
    "parameter’s ``.requires_grad`` attributes to False. Then the\n",
    "reinitialized layer’s parameters have ``.requires_grad=True`` by\n",
    "default. So now we know that *all parameters that have\n",
    ".requires_grad=True should be optimized.* Next, we make a list of such\n",
    "parameters and input this list to the SGD algorithm constructor.\n",
    "\n",
    "To verify this, check out the printed parameters to learn. When\n",
    "finetuning, this list should be long and include all of the model\n",
    "parameters. However, when feature extracting this list should be short\n",
    "and only include the weights and biases of the reshaped layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t features.conv0.weight\n",
      "\t features.norm0.weight\n",
      "\t features.norm0.bias\n",
      "\t features.denseblock1.denselayer1.norm1.weight\n",
      "\t features.denseblock1.denselayer1.norm1.bias\n",
      "\t features.denseblock1.denselayer1.conv1.weight\n",
      "\t features.denseblock1.denselayer1.norm2.weight\n",
      "\t features.denseblock1.denselayer1.norm2.bias\n",
      "\t features.denseblock1.denselayer1.conv2.weight\n",
      "\t features.denseblock1.denselayer2.norm1.weight\n",
      "\t features.denseblock1.denselayer2.norm1.bias\n",
      "\t features.denseblock1.denselayer2.conv1.weight\n",
      "\t features.denseblock1.denselayer2.norm2.weight\n",
      "\t features.denseblock1.denselayer2.norm2.bias\n",
      "\t features.denseblock1.denselayer2.conv2.weight\n",
      "\t features.denseblock1.denselayer3.norm1.weight\n",
      "\t features.denseblock1.denselayer3.norm1.bias\n",
      "\t features.denseblock1.denselayer3.conv1.weight\n",
      "\t features.denseblock1.denselayer3.norm2.weight\n",
      "\t features.denseblock1.denselayer3.norm2.bias\n",
      "\t features.denseblock1.denselayer3.conv2.weight\n",
      "\t features.denseblock1.denselayer4.norm1.weight\n",
      "\t features.denseblock1.denselayer4.norm1.bias\n",
      "\t features.denseblock1.denselayer4.conv1.weight\n",
      "\t features.denseblock1.denselayer4.norm2.weight\n",
      "\t features.denseblock1.denselayer4.norm2.bias\n",
      "\t features.denseblock1.denselayer4.conv2.weight\n",
      "\t features.denseblock1.denselayer5.norm1.weight\n",
      "\t features.denseblock1.denselayer5.norm1.bias\n",
      "\t features.denseblock1.denselayer5.conv1.weight\n",
      "\t features.denseblock1.denselayer5.norm2.weight\n",
      "\t features.denseblock1.denselayer5.norm2.bias\n",
      "\t features.denseblock1.denselayer5.conv2.weight\n",
      "\t features.denseblock1.denselayer6.norm1.weight\n",
      "\t features.denseblock1.denselayer6.norm1.bias\n",
      "\t features.denseblock1.denselayer6.conv1.weight\n",
      "\t features.denseblock1.denselayer6.norm2.weight\n",
      "\t features.denseblock1.denselayer6.norm2.bias\n",
      "\t features.denseblock1.denselayer6.conv2.weight\n",
      "\t features.transition1.norm.weight\n",
      "\t features.transition1.norm.bias\n",
      "\t features.transition1.conv.weight\n",
      "\t features.denseblock2.denselayer1.norm1.weight\n",
      "\t features.denseblock2.denselayer1.norm1.bias\n",
      "\t features.denseblock2.denselayer1.conv1.weight\n",
      "\t features.denseblock2.denselayer1.norm2.weight\n",
      "\t features.denseblock2.denselayer1.norm2.bias\n",
      "\t features.denseblock2.denselayer1.conv2.weight\n",
      "\t features.denseblock2.denselayer2.norm1.weight\n",
      "\t features.denseblock2.denselayer2.norm1.bias\n",
      "\t features.denseblock2.denselayer2.conv1.weight\n",
      "\t features.denseblock2.denselayer2.norm2.weight\n",
      "\t features.denseblock2.denselayer2.norm2.bias\n",
      "\t features.denseblock2.denselayer2.conv2.weight\n",
      "\t features.denseblock2.denselayer3.norm1.weight\n",
      "\t features.denseblock2.denselayer3.norm1.bias\n",
      "\t features.denseblock2.denselayer3.conv1.weight\n",
      "\t features.denseblock2.denselayer3.norm2.weight\n",
      "\t features.denseblock2.denselayer3.norm2.bias\n",
      "\t features.denseblock2.denselayer3.conv2.weight\n",
      "\t features.denseblock2.denselayer4.norm1.weight\n",
      "\t features.denseblock2.denselayer4.norm1.bias\n",
      "\t features.denseblock2.denselayer4.conv1.weight\n",
      "\t features.denseblock2.denselayer4.norm2.weight\n",
      "\t features.denseblock2.denselayer4.norm2.bias\n",
      "\t features.denseblock2.denselayer4.conv2.weight\n",
      "\t features.denseblock2.denselayer5.norm1.weight\n",
      "\t features.denseblock2.denselayer5.norm1.bias\n",
      "\t features.denseblock2.denselayer5.conv1.weight\n",
      "\t features.denseblock2.denselayer5.norm2.weight\n",
      "\t features.denseblock2.denselayer5.norm2.bias\n",
      "\t features.denseblock2.denselayer5.conv2.weight\n",
      "\t features.denseblock2.denselayer6.norm1.weight\n",
      "\t features.denseblock2.denselayer6.norm1.bias\n",
      "\t features.denseblock2.denselayer6.conv1.weight\n",
      "\t features.denseblock2.denselayer6.norm2.weight\n",
      "\t features.denseblock2.denselayer6.norm2.bias\n",
      "\t features.denseblock2.denselayer6.conv2.weight\n",
      "\t features.denseblock2.denselayer7.norm1.weight\n",
      "\t features.denseblock2.denselayer7.norm1.bias\n",
      "\t features.denseblock2.denselayer7.conv1.weight\n",
      "\t features.denseblock2.denselayer7.norm2.weight\n",
      "\t features.denseblock2.denselayer7.norm2.bias\n",
      "\t features.denseblock2.denselayer7.conv2.weight\n",
      "\t features.denseblock2.denselayer8.norm1.weight\n",
      "\t features.denseblock2.denselayer8.norm1.bias\n",
      "\t features.denseblock2.denselayer8.conv1.weight\n",
      "\t features.denseblock2.denselayer8.norm2.weight\n",
      "\t features.denseblock2.denselayer8.norm2.bias\n",
      "\t features.denseblock2.denselayer8.conv2.weight\n",
      "\t features.denseblock2.denselayer9.norm1.weight\n",
      "\t features.denseblock2.denselayer9.norm1.bias\n",
      "\t features.denseblock2.denselayer9.conv1.weight\n",
      "\t features.denseblock2.denselayer9.norm2.weight\n",
      "\t features.denseblock2.denselayer9.norm2.bias\n",
      "\t features.denseblock2.denselayer9.conv2.weight\n",
      "\t features.denseblock2.denselayer10.norm1.weight\n",
      "\t features.denseblock2.denselayer10.norm1.bias\n",
      "\t features.denseblock2.denselayer10.conv1.weight\n",
      "\t features.denseblock2.denselayer10.norm2.weight\n",
      "\t features.denseblock2.denselayer10.norm2.bias\n",
      "\t features.denseblock2.denselayer10.conv2.weight\n",
      "\t features.denseblock2.denselayer11.norm1.weight\n",
      "\t features.denseblock2.denselayer11.norm1.bias\n",
      "\t features.denseblock2.denselayer11.conv1.weight\n",
      "\t features.denseblock2.denselayer11.norm2.weight\n",
      "\t features.denseblock2.denselayer11.norm2.bias\n",
      "\t features.denseblock2.denselayer11.conv2.weight\n",
      "\t features.denseblock2.denselayer12.norm1.weight\n",
      "\t features.denseblock2.denselayer12.norm1.bias\n",
      "\t features.denseblock2.denselayer12.conv1.weight\n",
      "\t features.denseblock2.denselayer12.norm2.weight\n",
      "\t features.denseblock2.denselayer12.norm2.bias\n",
      "\t features.denseblock2.denselayer12.conv2.weight\n",
      "\t features.transition2.norm.weight\n",
      "\t features.transition2.norm.bias\n",
      "\t features.transition2.conv.weight\n",
      "\t features.denseblock3.denselayer1.norm1.weight\n",
      "\t features.denseblock3.denselayer1.norm1.bias\n",
      "\t features.denseblock3.denselayer1.conv1.weight\n",
      "\t features.denseblock3.denselayer1.norm2.weight\n",
      "\t features.denseblock3.denselayer1.norm2.bias\n",
      "\t features.denseblock3.denselayer1.conv2.weight\n",
      "\t features.denseblock3.denselayer2.norm1.weight\n",
      "\t features.denseblock3.denselayer2.norm1.bias\n",
      "\t features.denseblock3.denselayer2.conv1.weight\n",
      "\t features.denseblock3.denselayer2.norm2.weight\n",
      "\t features.denseblock3.denselayer2.norm2.bias\n",
      "\t features.denseblock3.denselayer2.conv2.weight\n",
      "\t features.denseblock3.denselayer3.norm1.weight\n",
      "\t features.denseblock3.denselayer3.norm1.bias\n",
      "\t features.denseblock3.denselayer3.conv1.weight\n",
      "\t features.denseblock3.denselayer3.norm2.weight\n",
      "\t features.denseblock3.denselayer3.norm2.bias\n",
      "\t features.denseblock3.denselayer3.conv2.weight\n",
      "\t features.denseblock3.denselayer4.norm1.weight\n",
      "\t features.denseblock3.denselayer4.norm1.bias\n",
      "\t features.denseblock3.denselayer4.conv1.weight\n",
      "\t features.denseblock3.denselayer4.norm2.weight\n",
      "\t features.denseblock3.denselayer4.norm2.bias\n",
      "\t features.denseblock3.denselayer4.conv2.weight\n",
      "\t features.denseblock3.denselayer5.norm1.weight\n",
      "\t features.denseblock3.denselayer5.norm1.bias\n",
      "\t features.denseblock3.denselayer5.conv1.weight\n",
      "\t features.denseblock3.denselayer5.norm2.weight\n",
      "\t features.denseblock3.denselayer5.norm2.bias\n",
      "\t features.denseblock3.denselayer5.conv2.weight\n",
      "\t features.denseblock3.denselayer6.norm1.weight\n",
      "\t features.denseblock3.denselayer6.norm1.bias\n",
      "\t features.denseblock3.denselayer6.conv1.weight\n",
      "\t features.denseblock3.denselayer6.norm2.weight\n",
      "\t features.denseblock3.denselayer6.norm2.bias\n",
      "\t features.denseblock3.denselayer6.conv2.weight\n",
      "\t features.denseblock3.denselayer7.norm1.weight\n",
      "\t features.denseblock3.denselayer7.norm1.bias\n",
      "\t features.denseblock3.denselayer7.conv1.weight\n",
      "\t features.denseblock3.denselayer7.norm2.weight\n",
      "\t features.denseblock3.denselayer7.norm2.bias\n",
      "\t features.denseblock3.denselayer7.conv2.weight\n",
      "\t features.denseblock3.denselayer8.norm1.weight\n",
      "\t features.denseblock3.denselayer8.norm1.bias\n",
      "\t features.denseblock3.denselayer8.conv1.weight\n",
      "\t features.denseblock3.denselayer8.norm2.weight\n",
      "\t features.denseblock3.denselayer8.norm2.bias\n",
      "\t features.denseblock3.denselayer8.conv2.weight\n",
      "\t features.denseblock3.denselayer9.norm1.weight\n",
      "\t features.denseblock3.denselayer9.norm1.bias\n",
      "\t features.denseblock3.denselayer9.conv1.weight\n",
      "\t features.denseblock3.denselayer9.norm2.weight\n",
      "\t features.denseblock3.denselayer9.norm2.bias\n",
      "\t features.denseblock3.denselayer9.conv2.weight\n",
      "\t features.denseblock3.denselayer10.norm1.weight\n",
      "\t features.denseblock3.denselayer10.norm1.bias\n",
      "\t features.denseblock3.denselayer10.conv1.weight\n",
      "\t features.denseblock3.denselayer10.norm2.weight\n",
      "\t features.denseblock3.denselayer10.norm2.bias\n",
      "\t features.denseblock3.denselayer10.conv2.weight\n",
      "\t features.denseblock3.denselayer11.norm1.weight\n",
      "\t features.denseblock3.denselayer11.norm1.bias\n",
      "\t features.denseblock3.denselayer11.conv1.weight\n",
      "\t features.denseblock3.denselayer11.norm2.weight\n",
      "\t features.denseblock3.denselayer11.norm2.bias\n",
      "\t features.denseblock3.denselayer11.conv2.weight\n",
      "\t features.denseblock3.denselayer12.norm1.weight\n",
      "\t features.denseblock3.denselayer12.norm1.bias\n",
      "\t features.denseblock3.denselayer12.conv1.weight\n",
      "\t features.denseblock3.denselayer12.norm2.weight\n",
      "\t features.denseblock3.denselayer12.norm2.bias\n",
      "\t features.denseblock3.denselayer12.conv2.weight\n",
      "\t features.denseblock3.denselayer13.norm1.weight\n",
      "\t features.denseblock3.denselayer13.norm1.bias\n",
      "\t features.denseblock3.denselayer13.conv1.weight\n",
      "\t features.denseblock3.denselayer13.norm2.weight\n",
      "\t features.denseblock3.denselayer13.norm2.bias\n",
      "\t features.denseblock3.denselayer13.conv2.weight\n",
      "\t features.denseblock3.denselayer14.norm1.weight\n",
      "\t features.denseblock3.denselayer14.norm1.bias\n",
      "\t features.denseblock3.denselayer14.conv1.weight\n",
      "\t features.denseblock3.denselayer14.norm2.weight\n",
      "\t features.denseblock3.denselayer14.norm2.bias\n",
      "\t features.denseblock3.denselayer14.conv2.weight\n",
      "\t features.denseblock3.denselayer15.norm1.weight\n",
      "\t features.denseblock3.denselayer15.norm1.bias\n",
      "\t features.denseblock3.denselayer15.conv1.weight\n",
      "\t features.denseblock3.denselayer15.norm2.weight\n",
      "\t features.denseblock3.denselayer15.norm2.bias\n",
      "\t features.denseblock3.denselayer15.conv2.weight\n",
      "\t features.denseblock3.denselayer16.norm1.weight\n",
      "\t features.denseblock3.denselayer16.norm1.bias\n",
      "\t features.denseblock3.denselayer16.conv1.weight\n",
      "\t features.denseblock3.denselayer16.norm2.weight\n",
      "\t features.denseblock3.denselayer16.norm2.bias\n",
      "\t features.denseblock3.denselayer16.conv2.weight\n",
      "\t features.denseblock3.denselayer17.norm1.weight\n",
      "\t features.denseblock3.denselayer17.norm1.bias\n",
      "\t features.denseblock3.denselayer17.conv1.weight\n",
      "\t features.denseblock3.denselayer17.norm2.weight\n",
      "\t features.denseblock3.denselayer17.norm2.bias\n",
      "\t features.denseblock3.denselayer17.conv2.weight\n",
      "\t features.denseblock3.denselayer18.norm1.weight\n",
      "\t features.denseblock3.denselayer18.norm1.bias\n",
      "\t features.denseblock3.denselayer18.conv1.weight\n",
      "\t features.denseblock3.denselayer18.norm2.weight\n",
      "\t features.denseblock3.denselayer18.norm2.bias\n",
      "\t features.denseblock3.denselayer18.conv2.weight\n",
      "\t features.denseblock3.denselayer19.norm1.weight\n",
      "\t features.denseblock3.denselayer19.norm1.bias\n",
      "\t features.denseblock3.denselayer19.conv1.weight\n",
      "\t features.denseblock3.denselayer19.norm2.weight\n",
      "\t features.denseblock3.denselayer19.norm2.bias\n",
      "\t features.denseblock3.denselayer19.conv2.weight\n",
      "\t features.denseblock3.denselayer20.norm1.weight\n",
      "\t features.denseblock3.denselayer20.norm1.bias\n",
      "\t features.denseblock3.denselayer20.conv1.weight\n",
      "\t features.denseblock3.denselayer20.norm2.weight\n",
      "\t features.denseblock3.denselayer20.norm2.bias\n",
      "\t features.denseblock3.denselayer20.conv2.weight\n",
      "\t features.denseblock3.denselayer21.norm1.weight\n",
      "\t features.denseblock3.denselayer21.norm1.bias\n",
      "\t features.denseblock3.denselayer21.conv1.weight\n",
      "\t features.denseblock3.denselayer21.norm2.weight\n",
      "\t features.denseblock3.denselayer21.norm2.bias\n",
      "\t features.denseblock3.denselayer21.conv2.weight\n",
      "\t features.denseblock3.denselayer22.norm1.weight\n",
      "\t features.denseblock3.denselayer22.norm1.bias\n",
      "\t features.denseblock3.denselayer22.conv1.weight\n",
      "\t features.denseblock3.denselayer22.norm2.weight\n",
      "\t features.denseblock3.denselayer22.norm2.bias\n",
      "\t features.denseblock3.denselayer22.conv2.weight\n",
      "\t features.denseblock3.denselayer23.norm1.weight\n",
      "\t features.denseblock3.denselayer23.norm1.bias\n",
      "\t features.denseblock3.denselayer23.conv1.weight\n",
      "\t features.denseblock3.denselayer23.norm2.weight\n",
      "\t features.denseblock3.denselayer23.norm2.bias\n",
      "\t features.denseblock3.denselayer23.conv2.weight\n",
      "\t features.denseblock3.denselayer24.norm1.weight\n",
      "\t features.denseblock3.denselayer24.norm1.bias\n",
      "\t features.denseblock3.denselayer24.conv1.weight\n",
      "\t features.denseblock3.denselayer24.norm2.weight\n",
      "\t features.denseblock3.denselayer24.norm2.bias\n",
      "\t features.denseblock3.denselayer24.conv2.weight\n",
      "\t features.transition3.norm.weight\n",
      "\t features.transition3.norm.bias\n",
      "\t features.transition3.conv.weight\n",
      "\t features.denseblock4.denselayer1.norm1.weight\n",
      "\t features.denseblock4.denselayer1.norm1.bias\n",
      "\t features.denseblock4.denselayer1.conv1.weight\n",
      "\t features.denseblock4.denselayer1.norm2.weight\n",
      "\t features.denseblock4.denselayer1.norm2.bias\n",
      "\t features.denseblock4.denselayer1.conv2.weight\n",
      "\t features.denseblock4.denselayer2.norm1.weight\n",
      "\t features.denseblock4.denselayer2.norm1.bias\n",
      "\t features.denseblock4.denselayer2.conv1.weight\n",
      "\t features.denseblock4.denselayer2.norm2.weight\n",
      "\t features.denseblock4.denselayer2.norm2.bias\n",
      "\t features.denseblock4.denselayer2.conv2.weight\n",
      "\t features.denseblock4.denselayer3.norm1.weight\n",
      "\t features.denseblock4.denselayer3.norm1.bias\n",
      "\t features.denseblock4.denselayer3.conv1.weight\n",
      "\t features.denseblock4.denselayer3.norm2.weight\n",
      "\t features.denseblock4.denselayer3.norm2.bias\n",
      "\t features.denseblock4.denselayer3.conv2.weight\n",
      "\t features.denseblock4.denselayer4.norm1.weight\n",
      "\t features.denseblock4.denselayer4.norm1.bias\n",
      "\t features.denseblock4.denselayer4.conv1.weight\n",
      "\t features.denseblock4.denselayer4.norm2.weight\n",
      "\t features.denseblock4.denselayer4.norm2.bias\n",
      "\t features.denseblock4.denselayer4.conv2.weight\n",
      "\t features.denseblock4.denselayer5.norm1.weight\n",
      "\t features.denseblock4.denselayer5.norm1.bias\n",
      "\t features.denseblock4.denselayer5.conv1.weight\n",
      "\t features.denseblock4.denselayer5.norm2.weight\n",
      "\t features.denseblock4.denselayer5.norm2.bias\n",
      "\t features.denseblock4.denselayer5.conv2.weight\n",
      "\t features.denseblock4.denselayer6.norm1.weight\n",
      "\t features.denseblock4.denselayer6.norm1.bias\n",
      "\t features.denseblock4.denselayer6.conv1.weight\n",
      "\t features.denseblock4.denselayer6.norm2.weight\n",
      "\t features.denseblock4.denselayer6.norm2.bias\n",
      "\t features.denseblock4.denselayer6.conv2.weight\n",
      "\t features.denseblock4.denselayer7.norm1.weight\n",
      "\t features.denseblock4.denselayer7.norm1.bias\n",
      "\t features.denseblock4.denselayer7.conv1.weight\n",
      "\t features.denseblock4.denselayer7.norm2.weight\n",
      "\t features.denseblock4.denselayer7.norm2.bias\n",
      "\t features.denseblock4.denselayer7.conv2.weight\n",
      "\t features.denseblock4.denselayer8.norm1.weight\n",
      "\t features.denseblock4.denselayer8.norm1.bias\n",
      "\t features.denseblock4.denselayer8.conv1.weight\n",
      "\t features.denseblock4.denselayer8.norm2.weight\n",
      "\t features.denseblock4.denselayer8.norm2.bias\n",
      "\t features.denseblock4.denselayer8.conv2.weight\n",
      "\t features.denseblock4.denselayer9.norm1.weight\n",
      "\t features.denseblock4.denselayer9.norm1.bias\n",
      "\t features.denseblock4.denselayer9.conv1.weight\n",
      "\t features.denseblock4.denselayer9.norm2.weight\n",
      "\t features.denseblock4.denselayer9.norm2.bias\n",
      "\t features.denseblock4.denselayer9.conv2.weight\n",
      "\t features.denseblock4.denselayer10.norm1.weight\n",
      "\t features.denseblock4.denselayer10.norm1.bias\n",
      "\t features.denseblock4.denselayer10.conv1.weight\n",
      "\t features.denseblock4.denselayer10.norm2.weight\n",
      "\t features.denseblock4.denselayer10.norm2.bias\n",
      "\t features.denseblock4.denselayer10.conv2.weight\n",
      "\t features.denseblock4.denselayer11.norm1.weight\n",
      "\t features.denseblock4.denselayer11.norm1.bias\n",
      "\t features.denseblock4.denselayer11.conv1.weight\n",
      "\t features.denseblock4.denselayer11.norm2.weight\n",
      "\t features.denseblock4.denselayer11.norm2.bias\n",
      "\t features.denseblock4.denselayer11.conv2.weight\n",
      "\t features.denseblock4.denselayer12.norm1.weight\n",
      "\t features.denseblock4.denselayer12.norm1.bias\n",
      "\t features.denseblock4.denselayer12.conv1.weight\n",
      "\t features.denseblock4.denselayer12.norm2.weight\n",
      "\t features.denseblock4.denselayer12.norm2.bias\n",
      "\t features.denseblock4.denselayer12.conv2.weight\n",
      "\t features.denseblock4.denselayer13.norm1.weight\n",
      "\t features.denseblock4.denselayer13.norm1.bias\n",
      "\t features.denseblock4.denselayer13.conv1.weight\n",
      "\t features.denseblock4.denselayer13.norm2.weight\n",
      "\t features.denseblock4.denselayer13.norm2.bias\n",
      "\t features.denseblock4.denselayer13.conv2.weight\n",
      "\t features.denseblock4.denselayer14.norm1.weight\n",
      "\t features.denseblock4.denselayer14.norm1.bias\n",
      "\t features.denseblock4.denselayer14.conv1.weight\n",
      "\t features.denseblock4.denselayer14.norm2.weight\n",
      "\t features.denseblock4.denselayer14.norm2.bias\n",
      "\t features.denseblock4.denselayer14.conv2.weight\n",
      "\t features.denseblock4.denselayer15.norm1.weight\n",
      "\t features.denseblock4.denselayer15.norm1.bias\n",
      "\t features.denseblock4.denselayer15.conv1.weight\n",
      "\t features.denseblock4.denselayer15.norm2.weight\n",
      "\t features.denseblock4.denselayer15.norm2.bias\n",
      "\t features.denseblock4.denselayer15.conv2.weight\n",
      "\t features.denseblock4.denselayer16.norm1.weight\n",
      "\t features.denseblock4.denselayer16.norm1.bias\n",
      "\t features.denseblock4.denselayer16.conv1.weight\n",
      "\t features.denseblock4.denselayer16.norm2.weight\n",
      "\t features.denseblock4.denselayer16.norm2.bias\n",
      "\t features.denseblock4.denselayer16.conv2.weight\n",
      "\t features.norm5.weight\n",
      "\t features.norm5.bias\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training and Validation Step\n",
    "--------------------------------\n",
    "\n",
    "Finally, the last step is to setup the loss for the model, then run the\n",
    "training and validation function for the set number of epochs. Notice,\n",
    "depending on the number of epochs this step may take a while on a CPU.\n",
    "Also, the default learning rate is not optimal for all of the models, so\n",
    "to achieve maximum accuracy it would be necessary to tune for each model\n",
    "separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 3.0542 Acc: 0.2881\n",
      "val Loss: 2.5167 Acc: 0.4653\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.4984 Acc: 0.4046\n",
      "val Loss: 2.1765 Acc: 0.5339\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 2.2487 Acc: 0.4598\n",
      "val Loss: 2.1190 Acc: 0.5562\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 2.1199 Acc: 0.4881\n",
      "val Loss: 2.0358 Acc: 0.5750\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.9810 Acc: 0.5202\n",
      "val Loss: 2.0986 Acc: 0.5879\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.9001 Acc: 0.5363\n",
      "val Loss: 1.9207 Acc: 0.6063\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.8066 Acc: 0.5595\n",
      "val Loss: 1.8457 Acc: 0.6218\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.7582 Acc: 0.5700\n",
      "val Loss: 1.7604 Acc: 0.6349\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.6933 Acc: 0.5844\n",
      "val Loss: 1.8154 Acc: 0.6314\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.6401 Acc: 0.5984\n",
      "val Loss: 1.7182 Acc: 0.6463\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.5801 Acc: 0.6092\n",
      "val Loss: 1.7902 Acc: 0.6568\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.5457 Acc: 0.6174\n",
      "val Loss: 1.6963 Acc: 0.6554\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.5104 Acc: 0.6275\n",
      "val Loss: 1.6492 Acc: 0.6638\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.4758 Acc: 0.6338\n",
      "val Loss: 1.5894 Acc: 0.6730\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.4452 Acc: 0.6415\n",
      "val Loss: 1.6421 Acc: 0.6681\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.4102 Acc: 0.6491\n",
      "val Loss: 1.5554 Acc: 0.6770\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.3779 Acc: 0.6581\n",
      "val Loss: 1.5544 Acc: 0.6829\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.3549 Acc: 0.6623\n",
      "val Loss: 1.5652 Acc: 0.6836\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.3269 Acc: 0.6665\n",
      "val Loss: 1.5715 Acc: 0.6836\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.3120 Acc: 0.6715\n",
      "val Loss: 1.4881 Acc: 0.6933\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.2883 Acc: 0.6778\n",
      "val Loss: 1.5074 Acc: 0.6906\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.2615 Acc: 0.6840\n",
      "val Loss: 1.4954 Acc: 0.6867\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.2371 Acc: 0.6884\n",
      "val Loss: 1.5236 Acc: 0.6939\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.2271 Acc: 0.6902\n",
      "val Loss: 1.5144 Acc: 0.6933\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.2027 Acc: 0.6977\n",
      "val Loss: 1.5594 Acc: 0.6875\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.1782 Acc: 0.7021\n",
      "val Loss: 1.4791 Acc: 0.7006\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.1661 Acc: 0.7051\n",
      "val Loss: 1.4756 Acc: 0.7040\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.1517 Acc: 0.7106\n",
      "val Loss: 1.4920 Acc: 0.6987\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.1406 Acc: 0.7111\n",
      "val Loss: 1.4788 Acc: 0.6940\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.1167 Acc: 0.7177\n",
      "val Loss: 1.4884 Acc: 0.7094\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.1021 Acc: 0.7184\n",
      "val Loss: 1.4553 Acc: 0.7042\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.0843 Acc: 0.7249\n",
      "val Loss: 1.4170 Acc: 0.7137\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.0698 Acc: 0.7269\n",
      "val Loss: 1.3945 Acc: 0.7088\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.0524 Acc: 0.7309\n",
      "val Loss: 1.4255 Acc: 0.7113\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.0430 Acc: 0.7347\n",
      "val Loss: 1.3721 Acc: 0.7114\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.0232 Acc: 0.7380\n",
      "val Loss: 1.4547 Acc: 0.7072\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.0185 Acc: 0.7392\n",
      "val Loss: 1.4702 Acc: 0.7089\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.9979 Acc: 0.7457\n",
      "val Loss: 1.3497 Acc: 0.7218\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.9955 Acc: 0.7449\n",
      "val Loss: 1.4519 Acc: 0.7088\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.9784 Acc: 0.7492\n",
      "val Loss: 1.4398 Acc: 0.7065\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.9644 Acc: 0.7512\n",
      "val Loss: 1.3225 Acc: 0.7221\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.9486 Acc: 0.7550\n",
      "val Loss: 1.3980 Acc: 0.7127\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.9455 Acc: 0.7571\n",
      "val Loss: 1.3573 Acc: 0.7189\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.9315 Acc: 0.7615\n",
      "val Loss: 1.4423 Acc: 0.7156\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.9221 Acc: 0.7618\n",
      "val Loss: 1.4255 Acc: 0.7159\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.9197 Acc: 0.7631\n",
      "val Loss: 1.4089 Acc: 0.7071\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.8893 Acc: 0.7703\n",
      "val Loss: 1.3594 Acc: 0.7249\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.8979 Acc: 0.7679\n",
      "val Loss: 1.4120 Acc: 0.7205\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.8767 Acc: 0.7726\n",
      "val Loss: 1.3894 Acc: 0.7208\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.8698 Acc: 0.7748\n",
      "val Loss: 1.3567 Acc: 0.7213\n",
      "\n",
      "Training complete in 981m 34s\n",
      "Best val Acc: 0.724884\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.7855 Acc: 0.8285\n"
     ]
    }
   ],
   "source": [
    "model_ft.eval()   # Set model to evaluate mode\n",
    "\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "phase = 'test'\n",
    "# Iterate over data.\n",
    "for inputs, labels in dataloaders_dict[phase]:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer_ft.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    # track history if only in train\n",
    "    with torch.set_grad_enabled(phase == 'train'):\n",
    "        outputs = model_ft(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / len(dataloaders_dict[phase].dataset)\n",
    "epoch_acc = running_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'densenet_82_85.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
