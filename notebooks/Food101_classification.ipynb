{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pillow\n",
    "!pip install PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "from scipy.misc.pilutil import imresize\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import keras\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# from batch_renorm import BatchRenormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple_pie\t    eggs_benedict\t     onion_rings\r\n",
      "baby_back_ribs\t    escargots\t\t     oysters\r\n",
      "baklava\t\t    falafel\t\t     pad_thai\r\n",
      "beef_carpaccio\t    filet_mignon\t     paella\r\n",
      "beef_tartare\t    fish_and_chips\t     pancakes\r\n",
      "beet_salad\t    foie_gras\t\t     panna_cotta\r\n",
      "beignets\t    french_fries\t     peking_duck\r\n",
      "bibimbap\t    french_onion_soup\t     pho\r\n",
      "bread_pudding\t    french_toast\t     pizza\r\n",
      "breakfast_burrito   fried_calamari\t     pork_chop\r\n",
      "bruschetta\t    fried_rice\t\t     poutine\r\n",
      "caesar_salad\t    frozen_yogurt\t     prime_rib\r\n",
      "cannoli\t\t    garlic_bread\t     pulled_pork_sandwich\r\n",
      "caprese_salad\t    gnocchi\t\t     ramen\r\n",
      "carrot_cake\t    greek_salad\t\t     ravioli\r\n",
      "ceviche\t\t    grilled_cheese_sandwich  red_velvet_cake\r\n",
      "cheese_plate\t    grilled_salmon\t     risotto\r\n",
      "cheesecake\t    guacamole\t\t     samosa\r\n",
      "chicken_curry\t    gyoza\t\t     sashimi\r\n",
      "chicken_quesadilla  hamburger\t\t     scallops\r\n",
      "chicken_wings\t    hot_and_sour_soup\t     seaweed_salad\r\n",
      "chocolate_cake\t    hot_dog\t\t     shrimp_and_grits\r\n",
      "chocolate_mousse    huevos_rancheros\t     spaghetti_bolognese\r\n",
      "churros\t\t    hummus\t\t     spaghetti_carbonara\r\n",
      "clam_chowder\t    ice_cream\t\t     spring_rolls\r\n",
      "club_sandwich\t    lasagna\t\t     steak\r\n",
      "crab_cakes\t    lobster_bisque\t     strawberry_shortcake\r\n",
      "creme_brulee\t    lobster_roll_sandwich    sushi\r\n",
      "croque_madame\t    macaroni_and_cheese      tacos\r\n",
      "cup_cakes\t    macarons\t\t     takoyaki\r\n",
      "deviled_eggs\t    miso_soup\t\t     tiramisu\r\n",
      "donuts\t\t    mussels\t\t     tuna_tartare\r\n",
      "dumplings\t    nachos\t\t     waffles\r\n",
      "edamame\t\t    omelette\r\n"
     ]
    }
   ],
   "source": [
    "!ls /food101/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_ix = {}\n",
    "ix_to_class = {}\n",
    "with open('/food101/meta/classes.txt', 'r') as txt:\n",
    "    classes = [l.strip() for l in txt.readlines()]\n",
    "    class_to_ix = dict(zip(classes, range(len(classes))))\n",
    "    ix_to_class = dict(zip(range(len(classes)), classes))\n",
    "    class_to_ix = {v: k for k, v in ix_to_class.items()}\n",
    "sorted_class_to_ix = collections.OrderedDict(sorted(class_to_ix.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/food101/meta/train.json') as f:\n",
    "    train_files = json.load(f)\n",
    "\n",
    "with open('/food101/meta/test.json') as f:\n",
    "    test_files = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = {'filename':[], 'label':[]}\n",
    "df_test = {'filename':[], 'label':[]}\n",
    "for class_name in train_files:\n",
    "    for i in range(len(train_files[class_name])):\n",
    "        df_train['filename'].append('/food101/images/' + str(train_files[class_name][i]) + '.jpg')\n",
    "        df_train['label'].append(class_to_ix[class_name])\n",
    "df_train = pd.DataFrame(df_train)\n",
    "\n",
    "for class_name in test_files:\n",
    "    for i in range(len(test_files[class_name])):\n",
    "        df_test['filename'].append('/food101/images/' + str(test_files[class_name][i]) + '.jpg')\n",
    "        df_test['label'].append(class_to_ix[class_name])\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 101\n",
    "df_train = shuffle(df_train,random_state=0)\n",
    "dataset = list(df_train['filename'])\n",
    "y = list(df_train['label'])\n",
    "# x_train = dataset[:70000]\n",
    "# y_train = y[:70000]\n",
    "# x_valid = dataset[70000:]\n",
    "# y_valid = y[70000:]\n",
    "x_test = list(df_test['filename'])\n",
    "y_test = list(df_test['label'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(dataset, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.1)\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=n_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68175, 101)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 299\n",
    "img_width = 299\n",
    "img_channels = 3\n",
    "img_dim = (img_height, img_width, img_channels)\n",
    "n_classes = len(sorted_class_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Inception(input_shape=(128, 128,3),weight_path=None):\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=img_dim)\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=input_shape))\n",
    "    model.add(base_model)\n",
    "    model.add(AveragePooling2D(pool_size=(8,8)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes, init='glorot_uniform', W_regularizer=l2(.0005), activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(src, choice):\n",
    "    if choice == 0:\n",
    "        # Rotate 90\n",
    "        src = np.rot90(src, 1)\n",
    "    if choice == 1:\n",
    "        # flip vertically\n",
    "        src = np.flipud(src)\n",
    "    if choice == 2:\n",
    "        # Rotate 180\n",
    "        src = np.rot90(src, 2)\n",
    "    if choice == 3:\n",
    "        # flip horizontally\n",
    "        src = np.fliplr(src)\n",
    "    if choice == 4:\n",
    "        # Rotate 90 counter-clockwise\n",
    "        src = np.rot90(src, 3)\n",
    "    if choice == 5:\n",
    "        # Rotate 180 and flip horizontally\n",
    "        src = np.rot90(src, 2)\n",
    "        src = np.fliplr(src)\n",
    "    return src\n",
    "\n",
    "def random_crop(x, random_crop_size):\n",
    "    w, h = x.shape[0], x.shape[1]\n",
    "    rangew = (w - random_crop_size[0]) // 2\n",
    "    rangeh = (h - random_crop_size[1]) // 2\n",
    "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
    "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
    "    return x[offsetw:offsetw+random_crop_size[0], offseth:offseth+random_crop_size[1], :]\n",
    "\n",
    "\n",
    "def train_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(x_train), batch_size):\n",
    "            min_side = 299\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(x_train))\n",
    "            x_train_batch = x_train[start:end]\n",
    "            y_batch = y_train_cat[start:end]\n",
    "            for filepath, tags in zip(x_train_batch, y_batch):\n",
    "                img = cv2.imread(filepath)\n",
    "                if img.shape[0] < img_height:\n",
    "                    wpercent = (img_height/float(img.shape[0]))\n",
    "                    hsize = int((float(img.shape[1])*float(wpercent)))\n",
    "                    img = cv2.resize(img, (img_height, hsize))\n",
    "                if img.shape[1] < img_width:\n",
    "                    hpercent = (img_width/float(img.shape[1]))\n",
    "                    wsize = int((float(img.shape[0])*float(hpercent)))\n",
    "                    img = cv2.resize(img, (wsize, img_width))                \n",
    "                img = random_crop(img, img_dim)\n",
    "                img = augment(img, np.random.randint(6))\n",
    "#                 img = cv2.resize(img, (img_width, img_height))\n",
    "                img = preprocess_input(img)\n",
    "                x_batch.append(img)\n",
    "            x_batch = np.array(x_batch, np.float32) / 255.\n",
    "            y_batch = np.array(y_batch, np.uint8)\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "def valid_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(x_valid), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(x_valid))\n",
    "            x_valid_batch = x_valid[start:end]\n",
    "            y_batch = y_valid_cat[start:end]\n",
    "            for filepath, tags in zip(x_valid_batch, y_batch):\n",
    "                img = cv2.imread(filepath)\n",
    "                if img.shape[0] < img_height:\n",
    "                    wpercent = (img_height/float(img.shape[0]))\n",
    "                    hsize = int((float(img.shape[1])*float(wpercent)))\n",
    "                    img = cv2.resize(img, (img_height, hsize))\n",
    "                if img.shape[1] < img_width:\n",
    "                    hpercent = (img_width/float(img.shape[1]))\n",
    "                    wsize = int((float(img.shape[0])*float(hpercent)))\n",
    "                    img = cv2.resize(img, (wsize, img_width))\n",
    "                img = random_crop(img, img_dim)\n",
    "                img = augment(img, np.random.randint(6))\n",
    "#                 img = cv2.resize(img, (img_width, img_height))\n",
    "                img = preprocess_input(img)\n",
    "                x_batch.append(img)\n",
    "            x_batch = np.array(x_batch, np.float32) / 255.\n",
    "            y_batch = np.array(y_batch, np.uint8)\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "def test_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(x_test), batch_size):\n",
    "            x_batch = []\n",
    "            end = min(start + batch_size, len(x_test))\n",
    "            test_batch = x_test[start:end]\n",
    "#             y_batch = y_test_cat[start:end]\n",
    "            for filepath in test_batch:\n",
    "                img = cv2.imread(filepath)\n",
    "                if img.shape[0] < img_height:\n",
    "                    img = cv2.resize(img, (img_height, img.shape[1]))\n",
    "                if img.shape[1] < img_width:\n",
    "                    img = cv2.resize(img, (img.shape[0], img_width))\n",
    "#                 img = random_crop(img, img_dim)\n",
    "#                 img = augment(img, np.random.randint(6))\n",
    "                img = cv2.resize(img, (img_width, img_height))\n",
    "                img = preprocess_input(img)\n",
    "                x_batch.append(img)\n",
    "            x_batch = np.array(x_batch, np.float32) / 255.\n",
    "#             y_batch = np.array(y_batch, np.uint8)\n",
    "#             yield x_batch, y_batch\n",
    "            yield x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68175 68175 7575 7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(101, kernel_regularizer=<keras.reg..., activation=\"softmax\", kernel_initializer=\"glorot_uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_760 (Bat (None, 299, 299, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 8, 8, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "average_pooling2d_80 (Averag (None, 1, 1, 2048)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1, 1, 2048)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 101)               206949    \n",
      "=================================================================\n",
      "Total params: 22,009,745\n",
      "Trainable params: 21,975,307\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1066/1066 [==============================] - 676s 634ms/step - loss: 2.4722 - acc: 0.4234 - val_loss: 2.5206 - val_acc: 0.4251\n",
      "Epoch 2/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 1.6915 - acc: 0.5934 - val_loss: 1.9424 - val_acc: 0.5353\n",
      "Epoch 3/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 1.4441 - acc: 0.6514 - val_loss: 1.8467 - val_acc: 0.5658\n",
      "Epoch 4/32\n",
      "1066/1066 [==============================] - 585s 548ms/step - loss: 1.2724 - acc: 0.6921 - val_loss: 1.7474 - val_acc: 0.5822\n",
      "Epoch 5/32\n",
      "1066/1066 [==============================] - 585s 548ms/step - loss: 1.1488 - acc: 0.7202 - val_loss: 1.6369 - val_acc: 0.6201\n",
      "Epoch 6/32\n",
      "1066/1066 [==============================] - 585s 548ms/step - loss: 1.0521 - acc: 0.7450 - val_loss: 1.6529 - val_acc: 0.6137\n",
      "Epoch 7/32\n",
      "1066/1066 [==============================] - 587s 550ms/step - loss: 0.9562 - acc: 0.7685 - val_loss: 1.5559 - val_acc: 0.6397\n",
      "Epoch 8/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 0.8696 - acc: 0.7905 - val_loss: 1.4569 - val_acc: 0.6657\n",
      "Epoch 9/32\n",
      "1066/1066 [==============================] - 585s 548ms/step - loss: 0.8053 - acc: 0.8063 - val_loss: 1.5461 - val_acc: 0.6467\n",
      "Epoch 10/32\n",
      "1066/1066 [==============================] - 584s 548ms/step - loss: 0.7457 - acc: 0.8232 - val_loss: 1.4001 - val_acc: 0.6771\n",
      "Epoch 11/32\n",
      "1066/1066 [==============================] - 585s 548ms/step - loss: 0.6830 - acc: 0.8388 - val_loss: 1.5552 - val_acc: 0.6498\n",
      "Epoch 12/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 0.6357 - acc: 0.8520 - val_loss: 1.5662 - val_acc: 0.6520\n",
      "Epoch 13/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 0.5819 - acc: 0.8655 - val_loss: 1.4957 - val_acc: 0.6659\n",
      "Epoch 14/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 0.5301 - acc: 0.8797 - val_loss: 1.5780 - val_acc: 0.6628\n",
      "Epoch 15/32\n",
      "1066/1066 [==============================] - 591s 555ms/step - loss: 0.4954 - acc: 0.8888 - val_loss: 1.4698 - val_acc: 0.6808\n",
      "Epoch 16/32\n",
      "1066/1066 [==============================] - 12800s 12s/step - loss: 0.3430 - acc: 0.9363 - val_loss: 1.1800 - val_acc: 0.7502\n",
      "Epoch 17/32\n",
      "1066/1066 [==============================] - 6179s 6s/step - loss: 0.2814 - acc: 0.9551 - val_loss: 1.1671 - val_acc: 0.7519\n",
      "Epoch 18/32\n",
      "1066/1066 [==============================] - 657s 616ms/step - loss: 0.2567 - acc: 0.9627 - val_loss: 1.1842 - val_acc: 0.7551\n",
      "Epoch 19/32\n",
      "1066/1066 [==============================] - 586s 550ms/step - loss: 0.2366 - acc: 0.9681 - val_loss: 1.1651 - val_acc: 0.7552\n",
      "Epoch 20/32\n",
      "1066/1066 [==============================] - 586s 550ms/step - loss: 0.2217 - acc: 0.9725 - val_loss: 1.1604 - val_acc: 0.7582\n",
      "Epoch 21/32\n",
      "1066/1066 [==============================] - 586s 549ms/step - loss: 0.2082 - acc: 0.9763 - val_loss: 1.1868 - val_acc: 0.7568\n",
      "Epoch 22/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 0.1987 - acc: 0.9788 - val_loss: 1.1666 - val_acc: 0.7589\n",
      "Epoch 23/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 0.1906 - acc: 0.9808 - val_loss: 1.1895 - val_acc: 0.7521\n",
      "Epoch 24/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 0.1820 - acc: 0.9823 - val_loss: 1.1942 - val_acc: 0.7521\n",
      "Epoch 25/32\n",
      "1066/1066 [==============================] - 586s 549ms/step - loss: 0.1756 - acc: 0.9837 - val_loss: 1.1944 - val_acc: 0.7521\n",
      "Epoch 26/32\n",
      "1066/1066 [==============================] - 586s 549ms/step - loss: 0.1679 - acc: 0.9857 - val_loss: 1.1956 - val_acc: 0.7537\n",
      "Epoch 27/32\n",
      "1066/1066 [==============================] - 1554s 1s/step - loss: 0.1634 - acc: 0.9865 - val_loss: 1.1793 - val_acc: 0.7564\n",
      "Epoch 28/32\n",
      "1066/1066 [==============================] - 625s 586ms/step - loss: 0.1572 - acc: 0.9877 - val_loss: 1.1885 - val_acc: 0.7513\n",
      "Epoch 29/32\n",
      "1066/1066 [==============================] - 584s 548ms/step - loss: 0.1490 - acc: 0.9903 - val_loss: 1.1891 - val_acc: 0.7530\n",
      "Epoch 30/32\n",
      "1066/1066 [==============================] - 585s 549ms/step - loss: 0.1472 - acc: 0.9907 - val_loss: 1.1867 - val_acc: 0.7549\n",
      "Epoch 31/32\n",
      "1066/1066 [==============================] - 1226s 1s/step - loss: 0.1455 - acc: 0.9907 - val_loss: 1.1918 - val_acc: 0.7530\n",
      "Epoch 32/32\n",
      "1066/1066 [==============================] - 614s 576ms/step - loss: 0.1451 - acc: 0.9911 - val_loss: 1.1814 - val_acc: 0.7575\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 32\n",
    "img_size = (img_height, img_width)\n",
    "\n",
    "i = 1\n",
    "with tf.device('/device:GPU:0'):\n",
    "    print len(x_train), len(y_train), len(x_valid), len(y_valid)\n",
    "\n",
    "    def schedule(epoch):\n",
    "        if epoch < 15:\n",
    "            return .01\n",
    "        elif epoch < 28:\n",
    "            return .002\n",
    "        else:\n",
    "            return .0004\n",
    "    \n",
    "    lr_scheduler = LearningRateScheduler(schedule)\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=1, verbose=1, min_lr=1e-7),]\n",
    "#              ModelCheckpoint(filepath='/Invasive_Species/weights/test_ap.fold_' + str(i) + '.hdf5', verbose=1,\n",
    "#                              save_best_only=True, save_weights_only=True, mode='auto')]\n",
    "\n",
    "    train_steps = len(x_train) / batch_size\n",
    "    valid_steps = len(x_valid) / batch_size\n",
    "    test_steps = len(x_test) / batch_size\n",
    "\n",
    "    model2 = Model_Inception(img_dim)\n",
    "    print model2.summary()\n",
    "\n",
    "    opt = SGD(lr=.01, momentum=.9)\n",
    "    model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model2.fit_generator(train_generator(), train_steps+1, epochs=epochs, verbose=1, callbacks=[lr_scheduler], validation_data=valid_generator(), validation_steps=valid_steps+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running validation predictions on fold {}'.format(i))\n",
    "preds_valid = model2.predict_generator(generator=valid_generator(),\n",
    "                              steps=valid_steps, verbose=1)\n",
    "\n",
    "print('Running train predictions on fold {}'.format(i))\n",
    "preds_train = model2.predict_generator(generator=train_generator(),\n",
    "                              steps=train_steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8695733229193356, 0.8135445544648878]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate_generator(test_generator(), steps=test_steps+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model2.save('food_inception_79_42.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(x, center_crop_size, **kwargs):\n",
    "    centerw, centerh = x.shape[0]//2, x.shape[1]//2\n",
    "    halfw, halfh = center_crop_size[0]//2, center_crop_size[1]//2\n",
    "    return x[centerw-halfw:centerw+halfw+1,centerh-halfh:centerh+halfh+1, :]\n",
    "\n",
    "def predict_10_crop(img, ix, top_n=5, plot=False, preprocess=True, debug=False):\n",
    "    img=cv2.imread(img)\n",
    "    w, h, _ = img.shape\n",
    "    if img.shape[0] < img_height:\n",
    "        img = cv2.resize(img, (img_height, img.shape[1]))\n",
    "    if img.shape[1] < img_width:\n",
    "        img = cv2.resize(img, (img.shape[0], img_width))\n",
    "    flipped_X = np.fliplr(img)\n",
    "    crops = [\n",
    "        img[:299,:299, :], # Upper Left\n",
    "        img[:299, img.shape[1]-299:, :], # Upper Right\n",
    "        img[img.shape[0]-299:, :299, :], # Lower Left\n",
    "        img[img.shape[0]-299:, img.shape[1]-299:, :], # Lower Right\n",
    "        center_crop(img, (299, 299)),\n",
    "        \n",
    "        flipped_X[:299,:299, :],\n",
    "        flipped_X[:299, flipped_X.shape[1]-299:, :],\n",
    "        flipped_X[flipped_X.shape[0]-299:, :299, :],\n",
    "        flipped_X[flipped_X.shape[0]-299:, flipped_X.shape[1]-299:, :],\n",
    "        center_crop(flipped_X, (299, 299))\n",
    "    ]\n",
    "    if preprocess:\n",
    "        crops = [preprocess_input(x) for x in crops]\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
    "        ax[0][0].imshow(crops[0])\n",
    "        ax[0][1].imshow(crops[1])\n",
    "        ax[0][2].imshow(crops[2])\n",
    "        ax[0][3].imshow(crops[3])\n",
    "        ax[0][4].imshow(crops[4])\n",
    "        ax[1][0].imshow(crops[5])\n",
    "        ax[1][1].imshow(crops[6])\n",
    "        ax[1][2].imshow(crops[7])\n",
    "        ax[1][3].imshow(crops[8])\n",
    "        ax[1][4].imshow(crops[9])\n",
    "    try:\n",
    "        crops = np.array(crops, np.float32) / 255.\n",
    "        y_pred = model2.predict(crops)\n",
    "        preds = np.argmax(y_pred, axis=1)\n",
    "        top_n_preds= np.argpartition(y_pred, -top_n)[:,-top_n:]\n",
    "    except:\n",
    "        print np.array(crops).shape, ix\n",
    "    if debug:\n",
    "        print('Top-1 Predicted:', preds)\n",
    "        print('Top-5 Predicted:', top_n_preds)\n",
    "        print('True Label:', y_test[ix])\n",
    "    return preds, top_n_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "CPU times: user 13min 39s, sys: 42.7 s, total: 14min 22s\n",
      "Wall time: 23min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preds_10_crop = {}\n",
    "for ix in range(len(x_test)):\n",
    "    if ix % 1000 == 0:\n",
    "        print(ix)\n",
    "    preds_10_crop[ix] = predict_10_crop(x_test[ix], ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Number of unique predictions per image')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_uniq = {k: np.unique(v[0]) for k, v in preds_10_crop.items()}\n",
    "preds_hist = np.array([len(x) for x in preds_uniq.values()])\n",
    "\n",
    "plt.hist(preds_hist, bins=11)\n",
    "plt.title('Number of unique predictions per image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_top_1 = {k: collections.Counter(v[0]).most_common(1) for k, v in preds_10_crop.items()}\n",
    "\n",
    "top_5_per_ix = {k: collections.Counter(preds_10_crop[k][1].reshape(-1)).most_common(5) \n",
    "                for k, v in preds_10_crop.items()}\n",
    "preds_top_5 = {k: [y[0] for y in v] for k, v in top_5_per_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy, 10-Crop: 84.93%\n",
      "CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms\n",
      "Wall time: 18.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "right_counter = 0\n",
    "for i in range(len(y_test)):\n",
    "    guess, actual = preds_top_1[i][0][0], y_test[i]\n",
    "    if guess == actual:\n",
    "        right_counter += 1\n",
    "        \n",
    "print('Top-1 Accuracy, 10-Crop: {0:.2f}%'.format(float(right_counter) / len(y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Accuracy, 10-Crop: 96.76%\n",
      "CPU times: user 17.8 ms, sys: 0 ns, total: 17.8 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_5_counter = 0\n",
    "for i in range(len(y_test)):\n",
    "    guesses, actual = preds_top_5[i], y_test[i]\n",
    "    if actual in guesses:\n",
    "        top_5_counter += 1\n",
    "        \n",
    "print('Top-5 Accuracy, 10-Crop: {0:.2f}%'.format(float(top_5_counter) / len(y_test) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
